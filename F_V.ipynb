{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fusion part\n"
     ]
    }
   ],
   "source": [
    "print(\"fusion part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/usama/anaconda3/lib/python3.10/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "#vocal library import\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment, effects\n",
    "import os\n",
    "\n",
    "import speech_recognition as src\n",
    "from pydub.silence import split_on_silence\n",
    "import wavio as wv\n",
    "from scipy.io.wavfile import write, read\n",
    "import sounddevice as sd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# face library import\n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "model = tf.keras.models.load_model('saved_training_50epo.h5')\n",
    "model_v = tf.keras.models.load_model('my_model_weights_100epo.h5')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processing the voice to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       path\n",
       "0  test.wav"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path = 'test.wav'\n",
    "dff = pd.DataFrame([test_path], columns=['path']),\n",
    "df = pd.concat(dff, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(path):\n",
    "    _, sr = librosa.load(path)\n",
    " \n",
    "    raw_audio = AudioSegment.from_file(path)\n",
    "    \n",
    "    samples = np.array(raw_audio.get_array_of_samples(), dtype='float32')\n",
    "    trimmed, _ = librosa.effects.trim(samples, top_db=25)\n",
    "    padded = np.pad(trimmed, (0, 180000-len(trimmed)), 'constant')\n",
    "    return padded, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = src.Recognizer()\n",
    "def transcribe_audio(path):\n",
    "    # use the audio file as the audio source\n",
    "    with src.AudioFile(path) as source:\n",
    "        audio_listened = r.record(source)\n",
    "        # try converting it to text\n",
    "        text = r.recognize_google(audio_listened)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_large_audio_transcription_on_silence(path):\n",
    "    \"\"\"Splitting the large audio file into chunks\n",
    "    and apply speech recognition on each of these chunks\"\"\"\n",
    "    # open the audio file using pydub\n",
    "    sound = AudioSegment.from_file(path)  \n",
    "    # split audio sound where silence is 500 miliseconds or more and get chunks\n",
    "    chunks = split_on_silence(sound,\n",
    "        # experiment with this value for your target audio file\n",
    "        min_silence_len = 500,\n",
    "        # adjust this per requirement\n",
    "        silence_thresh = sound.dBFS-14,\n",
    "        # keep the silence for 1 second, adjustable as well\n",
    "        keep_silence=500,\n",
    "    )\n",
    "    folder_name = \"audio-chunks\"\n",
    "    # create a directory to store the audio chunks\n",
    "    if not os.path.isdir(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "    whole_text = \"\"\n",
    "    # process each chunk \n",
    "    for i, audio_chunk in enumerate(chunks, start=1):\n",
    "        # export audio chunk and save it in\n",
    "        # the `folder_name` directory.\n",
    "        chunk_filename = os.path.join(folder_name, f\"chunk{i}.wav\")\n",
    "        audio_chunk.export(chunk_filename, format=\"wav\")\n",
    "        # recognize the chunk\n",
    "        try:\n",
    "            text = transcribe_audio(chunk_filename)\n",
    "        except src.UnknownValueError as e:\n",
    "            print(\"Error:\", str(e))\n",
    "        else:\n",
    "            text = f\"{text.capitalize()}. \"\n",
    "            print(chunk_filename, \":\", text)\n",
    "            whole_text += text\n",
    "    # return the text for all chunks detected\n",
    "    return whole_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "# import time\n",
    "# timeout = time.time() + 18  # 18 seconds from now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_func():\n",
    "    print(\"Thread Vision started\")\n",
    "    test_img = cv2.imread('cry.jpg')\n",
    "    gray_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray_img, 1.1,4)\n",
    "    for x,y,z,h in faces:\n",
    "        r_gray = gray_img[y:y+h, x:x+z]\n",
    "        r_colour = test_img[y:y+h, x:x+z]\n",
    "        cv2.rectangle(test_img, (x,y), (x+z, y+h), (0,255,0), 2)\n",
    "        faces1 = face_cascade.detectMultiScale(r_gray)\n",
    "        if len(faces1) == 0:\n",
    "            print(\"no face detected\")\n",
    "        else:\n",
    "            for a,b,c,d in faces1:\n",
    "                face_r = r_colour[b:b+d, a:a+c]\n",
    "    #plt.imshow(cv2.cvtColor(face_r, cv2.COLOR_BGR2BGRA))\n",
    "    final_img = cv2.resize(face_r, (224,224))\n",
    "    final_img = np.expand_dims(final_img,axis=0)\n",
    "    final_img = final_img/255\n",
    "    Predictions = model.predict(final_img)\n",
    "    Predictions = np.argmax(Predictions)\n",
    "   \n",
    "    font_scale=1.5\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    # set the rectangle background to white\n",
    "    rectangle_bgr = (255, 255, 255)\n",
    "    # make a black image\n",
    "    img = np.zeros((500, 500))\n",
    "    # set some text\n",
    "    text = \"Some text in a box!\"\n",
    "    # get the width and height of the text box\n",
    "    (text_width, text_height) = cv2.getTextSize(text, font, fontScale=font_scale, thickness=1)[0] # set the text start position\n",
    "    text_offset_x = 10\n",
    "    text_offset_y= img.shape[0] - 25\n",
    "    #make the coords of the box with a small padding of two pixels\n",
    "    box_coords = ((text_offset_x, text_offset_y), (text_offset_x + text_width+2, text_offset_y - text_height - 2)) \n",
    "    cv2.rectangle(img, box_coords[0], box_coords[1], rectangle_bgr, cv2.FILLED)\n",
    "    cv2.putText(img, text, (text_offset_x, text_offset_y), font, fontScale=font_scale, color=(0, 0, 0), thickness=1)\n",
    "\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    # Check if the webcam is opened correctly\n",
    "    if not cap.isOpened():\n",
    "        cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for i in range(80):\n",
    "        # if time.time() > timeout:\n",
    "        #     break\n",
    "        ret, frame = cap.read()\n",
    "        #eye_cascade = cv2.Cascade Classifier (cv2.data. haarcascades +'haarcascade_eye.xml')\n",
    "        faceCascade = cv2.CascadeClassifier (cv2.data.haarcascades + 'haarcascade_frontalface_default.xml') \n",
    "        gray_img = cv2.cvtColor (frame, cv2.COLOR_BGR2GRAY)\n",
    "        #print (faceCascade.empty())\n",
    "        faces = face_cascade.detectMultiScale(gray_img, 1.1,4)\n",
    "        for x,y,z,h in faces:\n",
    "            r_gray = gray_img[y:y+h, x:x+z]\n",
    "            r_colour = frame[y:y+h, x:x+z]\n",
    "            cv2.rectangle(test_img, (x,y), (x+z, y+h), (0,255,0), 2)\n",
    "            faces1 = face_cascade.detectMultiScale(r_gray)\n",
    "            if len(faces1) == 0:\n",
    "                print(\"no face detected\")\n",
    "            else:\n",
    "                for a,b,c,d in faces1:\n",
    "                    face_r = r_colour[b:b+d, a:a+c]\n",
    "\n",
    "        ## cropping the face\n",
    "        final_img = cv2.resize(face_r, (224,224))\n",
    "        final_img = np.expand_dims(final_img,axis=0)\n",
    "        final_img = final_img/255\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        Predictions = model.predict(final_img)\n",
    "\n",
    "        #save to csv filee\n",
    "        data.append(np.argmax(Predictions))\n",
    "        df_face = pd.DataFrame(data, columns=['Face'])\n",
    "        \n",
    "        df_face.to_csv('face_outputs.csv', index=False)\n",
    "\n",
    "\n",
    "        # pd.DataFrame(Predictions, columns=['face']).to_csv('prediction.csv')\n",
    "        font_scale = 1.5\n",
    "        font = cv2.FONT_HERSHEY_PLAIN\n",
    "        if (np.argmax (Predictions)==0): \n",
    "            status = \"Angry\"\n",
    "            x1,y1, w1,h1 = 0,0,175,75\n",
    "            # Draw black background rectangle\n",
    "            cv2.rectangle(frame, (x1, x1), (x1+ w1, y1 + h1), (0,0,0), -1)\n",
    "            # Add text\n",
    "            cv2.putText(frame, status, (x1+ int(w1/10), y1 + int (h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "            cv2.putText(frame, status, (100, 150), font, 3, (0, 0, 255), 2, cv2.LINE_4)\n",
    "            cv2.rectangle (frame, (x, y), (x+z, y+h), (0, 0, 255))\n",
    "\n",
    "        elif (np.argmax (Predictions)==1):\n",
    "            status = \"Disgust\"\n",
    "            x1,y1,w1,h1 = 0,0,175,75\n",
    "            # Draw black background rectangle\n",
    "            cv2.rectangle(frame, (x1, x1), (x1+ w1, y1 + h1), (0,0,0), -1)\n",
    "            # Add text\n",
    "            cv2.putText(frame, status, (x1+ int(w1/10), y1 + int (h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "            cv2.putText(frame, status, (100, 150), font, 3, (0, 0, 255),2,cv2.LINE_4)\n",
    "            cv2.rectangle(frame, (x, y), (x+z, y+h), (0, 0, 255))\n",
    "\n",
    "        elif (np.argmax (Predictions)==2):\n",
    "            status = \"Fear\"\n",
    "            x1,y1,w1,h1 = 0,0,175,75\n",
    "            # Draw black background rectangle\n",
    "            cv2.rectangle(frame, (x1, x1), (x1+ w1, y1 + h1), (0,0,0), -1)\n",
    "            # Add text\n",
    "            cv2.putText(frame, status, (x1+ int(w1/10), y1 + int (h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "            cv2.putText(frame, status, (100, 150), font, 3, (0, 0, 255),2,cv2.LINE_4)\n",
    "            cv2.rectangle(frame, (x, y), (x+z, y+h), (0, 0, 255))\n",
    "\n",
    "        elif (np.argmax (Predictions)==3):\n",
    "            status = \"Happy\"\n",
    "            x1,y1,w1,h1 = 0,0,175,75\n",
    "            # Draw black background rectangle\n",
    "            cv2.rectangle(frame, (x1, x1), (x1+ w1, y1 + h1), (0,0,0), -1)\n",
    "            # Add text\n",
    "            cv2.putText(frame, status, (x1+ int(w1/10), y1 + int (h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "            cv2.putText(frame, status, (100, 150), font, 3, (0, 0, 255),2,cv2.LINE_4)\n",
    "            cv2.rectangle(frame, (x, y), (x+z, y+h), (0, 0, 255))\n",
    "\n",
    "        elif (np.argmax (Predictions)==5):\n",
    "            status = \"Sad\"\n",
    "            x1,y1,w1,h1 = 0,0,175,75\n",
    "            # Draw black background rectangle\n",
    "            cv2.rectangle(frame, (x1, x1), (x1+ w1, y1 + h1), (0,0,0), -1)\n",
    "            # Add text\n",
    "            cv2.putText(frame, status, (x1+ int(w1/10), y1 + int (h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "            cv2.putText(frame, status, (100, 150), font, 3, (0, 0, 255),2,cv2.LINE_4)\n",
    "            cv2.rectangle(frame, (x, y), (x+z, y+h), (0, 0, 255))\n",
    "        \n",
    "        elif (np.argmax (Predictions)==6):\n",
    "            status = \"Surprised\"\n",
    "            x1,y1,w1,h1 = 0,0,175,75\n",
    "            # Draw black background rectangle\n",
    "            cv2.rectangle(frame, (x1, x1), (x1+ w1, y1 + h1), (0,0,0), -1)\n",
    "            # Add text\n",
    "            cv2.putText(frame, status, (x1+ int(w1/10), y1 + int (h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "            cv2.putText(frame, status, (100, 150), font, 3, (0, 0, 255),2,cv2.LINE_4)\n",
    "            cv2.rectangle(frame, (x, y), (x+z, y+h), (0, 0, 255))\n",
    "\n",
    "        else:\n",
    "            status = \"Neutral\"\n",
    "            x1,y1,w1,h1 = 0,0,175,75\n",
    "            # Draw black background rectangle\n",
    "            cv2.rectangle(frame, (x1, x1), (x1+ w1, y1 + h1), (0,0,0), -1)\n",
    "            # Add text\n",
    "            cv2.putText(frame, status, (x1+ int(w1/10), y1 + int (h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "            cv2.putText(frame, status, (100, 150), font, 3, (0, 0, 255),2,cv2.LINE_4)\n",
    "            cv2.rectangle(frame, (x, y), (x+z, y+h), (0, 0, 255))\n",
    "\n",
    "\n",
    "        cv2.imshow('face emotions',frame)\n",
    "        if cv2.waitKey(3) & 0xFF == ord('q'):\n",
    "            cv2.destroyWindow('face emotions')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thread_two_func():\n",
    "    print(\"Thread voice and text started\")\n",
    "    \n",
    "    print('say hi')\n",
    "\n",
    "    data_voice = []\n",
    "    \n",
    "    for i in range(4):\n",
    "        # if time.time() > timeout:\n",
    "        #     break\n",
    "\n",
    "        #init the fequancy and the time of the .wav file (duration of listening)\n",
    "        freq = 44100\n",
    "        duration = 4\n",
    "        zcr_list = []\n",
    "        rms_list = []\n",
    "        mfccs_list = []\n",
    "\n",
    "        #record a .wav file\n",
    "        recording = sd.rec(int(duration * freq), samplerate=freq, channels=1)\n",
    "        sd.wait()\n",
    "        write(\"recording0.wav\", freq, recording)\n",
    "        wv.write(\"recording1.wav\", recording, freq, sampwidth=2)\n",
    "        \n",
    "\n",
    "        test_path = 'recording1.wav'\n",
    "        \n",
    "        #converting the data from vocal to textual\n",
    "        text =  get_large_audio_transcription_on_silence(test_path)\n",
    "        print(text)\n",
    "\n",
    "        #get the senitmet from the text\n",
    "        sia = SentimentIntensityAnalyzer()\n",
    "        my_dict = sia.polarity_scores(text)\n",
    "\n",
    "        #get the value and the intensity of the text\n",
    "        max_value = max(my_dict.values())\n",
    "        max_key = max(my_dict, key=my_dict.get)\n",
    "        \n",
    "        dff = pd.DataFrame([test_path], columns=['path']),\n",
    "        df = pd.concat(dff, axis=1)\n",
    "        \n",
    "        #init the frame of the vocal \n",
    "        FRAME_LENGTH = 2048\n",
    "        HOP_LENGTH = 512\n",
    "\n",
    "        #iterrate through our .wav file that we recored\n",
    "        for index, row in df.iterrows():\n",
    "            try:\n",
    "                y, sr = preprocess_audio(row.path)\n",
    "                zcr = librosa.feature.zero_crossing_rate(y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n",
    "                rms = librosa.feature.rms(y=y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n",
    "                mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=HOP_LENGTH)\n",
    "\n",
    "                zcr_list.append(zcr)\n",
    "                rms_list.append(rms)\n",
    "                mfccs_list.append(mfccs)\n",
    "                 \n",
    "            except:\n",
    "                print(f\"Failed for path: {row.path}\")\n",
    "        \n",
    "        #preparing the input to the model\n",
    "        try:\n",
    "            X_test = np.concatenate((\n",
    "                np.swapaxes(zcr_list, 1, 2), \n",
    "                np.swapaxes(rms_list, 1, 2), \n",
    "                np.swapaxes(mfccs_list, 1, 2)), \n",
    "                axis=2\n",
    "            )\n",
    "            \n",
    "            X_test = X_test.astype('float32')\n",
    "            y_pred = np.argmax(model_v.predict(X_test), axis=1)\n",
    "            \n",
    "            #save to csv\n",
    "            data_voice.append([ y_pred,max_value, text,  max_key])\n",
    "            df_voice = pd.DataFrame(data_voice, columns=[ 'voice','text_intensity', 'text', 'text_sentiment' ])\n",
    "            \n",
    "            df_voice.to_csv('voice_outputs.csv', index=False)\n",
    "\n",
    "            \n",
    "\n",
    "        except:\n",
    "            print(\"no data have been passed\")\n",
    "        \n",
    "        emotion_dic = {'neutral' : 0, 'happy'   : 1,'sad'     : 2, 'angry'   : 3, 'fear'    : 4, 'disgust' : 5}\n",
    "    \n",
    "        print(list(emotion_dic.keys())[list(emotion_dic.values()).index(int(y_pred))])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread voice and text started\n",
      "say hi\n",
      "Thread Vision started\n",
      "1/1 [==============================] - 1s 527ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "no face detected\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/usama/anaconda3/lib/python3.10/site-packages/wavio.py:259: ClippedDataWarning: Some data values have been clipped.  With scale=1.0, the interval of input values that will not be clipped is [-1.0000305180437934, 1.0]\n",
      "  _warnings.warn(ClippedDataWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "audio-chunks/chunk1.wav : Stockport to my existence file. \n",
      "Stockport to my existence file. \n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "no face detected\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "angry\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "audio-chunks/chunk1.wav : Sorry it's a bit slow since at the moment. \n",
      "Sorry it's a bit slow since at the moment. \n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 233ms/step\n",
      "fear\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "no face detected\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/usama/anaconda3/lib/python3.10/site-packages/wavio.py:259: ClippedDataWarning: Some data values have been clipped.  With scale=1.0, the interval of input values that will not be clipped is [-1.0000305180437934, 1.0]\n",
      "  _warnings.warn(ClippedDataWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no face detected\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "Error: \n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "audio-chunks/chunk2.wav : Spotify. \n",
      "Spotify. \n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "angry\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "audio-chunks/chunk1.wav : It will automatically run through. \n",
      "It will automatically run through. \n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "fear\n",
      "Thread execution complete\n"
     ]
    }
   ],
   "source": [
    "# Create thread for alignment\n",
    "thread_two = threading.Thread(target=thread_two_func)\n",
    "\n",
    "# Start the thread\n",
    "thread_two.start()\n",
    "\n",
    "# Wait for threads to complete while continuing the parant \n",
    "face_func()\n",
    "thread_two.join()\n",
    "\n",
    "print(\"Thread execution complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>face_senti</th>\n",
       "      <th>voice</th>\n",
       "      <th>text_intensity</th>\n",
       "      <th>text</th>\n",
       "      <th>text_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000</td>\n",
       "      <td>Stockport to my existence file.</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.843</td>\n",
       "      <td>Sorry it's a bit slow since at the moment.</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000</td>\n",
       "      <td>Spotify.</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000</td>\n",
       "      <td>It will automatically run through.</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   face_senti  voice  text_intensity  \\\n",
       "0           4      3           1.000   \n",
       "1           3      4           0.843   \n",
       "2           4      3           1.000   \n",
       "3           3      4           1.000   \n",
       "\n",
       "                                          text text_sentiment  \n",
       "0             Stockport to my existence file.             neu  \n",
       "1  Sorry it's a bit slow since at the moment.             neu  \n",
       "2                                    Spotify.             neu  \n",
       "3          It will automatically run through.             neu  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process the csv files to have clean output\n",
    "\n",
    "v_df = pd.read_csv(\"voice_outputs.csv\")\n",
    "for index in v_df.index:\n",
    "    if v_df['text_intensity'][index] == 0.000:\n",
    "        v_df['text_sentiment'][index] = 'neu'\n",
    "\n",
    "v_df['voice'] = v_df['voice'].str[1]  \n",
    "v_df['voice'] = v_df['voice'].apply(lambda arr: arr[0]).apply(pd.to_numeric, downcast='integer')\n",
    "v_df.loc[v_df['text'].isnull(),'voice'] = 0\n",
    "\n",
    "\n",
    "#process and aligine face with voice data\n",
    "f_df = pd.read_csv('face_outputs.csv')\n",
    "num_groups = len(f_df) // 20\n",
    "grouped_df = [f_df.iloc[i * 4:(i + 1) * 4] for i in range(num_groups)]\n",
    "averages = [group['Face'].mode().iloc[0] for group in grouped_df]\n",
    "face_df = pd.DataFrame({'face_senti': averages})\n",
    "\n",
    "#concat all outputs in one dataframe\n",
    "final_df = pd.concat([face_df, v_df], axis=1)\n",
    "final_df.to_csv('final_collective_sentiments.csv', index=False)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed Neutral-Angry-Neutral\n",
      "Mixed Happy-Fearful-Neutral\n",
      "Mixed Neutral-Angry-Neutral\n",
      "Mixed Happy-Fearful-Neutral\n"
     ]
    }
   ],
   "source": [
    "def predict_emotion(face_emotion, voice_emotion, text_sentiment):\n",
    "    emotion_mapping = {\n",
    "        # Single Emotions\n",
    "        ('Happy', 'Happy', 'Positive'): 'Happy',\n",
    "        ('Sad', 'Sad', 'Negative'): 'Sad',\n",
    "        ('Angry', 'Angry', 'Negative'): 'Angry',\n",
    "        ('Fearful', 'Fearful', 'Negative'): 'Fearful',\n",
    "        ('Disgusted', 'Disgusted', 'Negative'): 'Disgusted',\n",
    "        ('Neutral', 'Neutral', 'Neutral'): 'Neutral',\n",
    "\n",
    "        # Mixed Emotions with happy face\n",
    "        ('Happy', 'Neutral', 'Positive'): 'Mixed Happy-Neutral-Positive',\n",
    "        ('Happy', 'Sad', 'Positive'): 'Mixed Happy-Sad-Positive',\n",
    "        ('Happy', 'Angry', 'Positive'): 'Mixed Happy-Angry-Positive',\n",
    "        ('Happy', 'Fearful', 'Positive'): 'Mixed Happy-Fearful-Positive',\n",
    "        ('Happy', 'Disgusted', 'Positive'): 'Mixed Happy-Disgusted-Positive',\n",
    "        ('Happy', 'Neutral', 'Neutral'): 'Mixed Happy-Neutral-Neutral',\n",
    "        ('Happy', 'Sad', 'Neutral'): 'Mixed Happy-Sad-Neutral',\n",
    "        ('Happy', 'Angry', 'Neutral'): 'Mixed Happy-Angry-Neutral',\n",
    "        ('Happy', 'Fearful', 'Neutral'): 'Mixed Happy-Fearful-Neutral',\n",
    "        ('Happy', 'Happy', 'Neutral'): 'Mixed Happy-Happy-Neutral',\n",
    "        ('Happy', 'Disgusted', 'Neutral'): 'Mixed Happy-Disgusted-Neutral',\n",
    "        ('Happy', 'Neutral', 'Negative'): 'Mixed Happy-Neutral-Negative',\n",
    "        ('Happy', 'Sad', 'Negative'): 'Mixed Happy-Sad-Negative',\n",
    "        ('Happy', 'Angry', 'Negative'): 'Mixed Happy-Angry-Negative',\n",
    "        ('Happy', 'Fearful', 'Negative'): 'Mixed Happy-Fearful-Negative',\n",
    "        ('Happy', 'Happy', 'Negative'): 'Mixed Happy-Happy-Negative',\n",
    "        ('Happy', 'Disgusted', 'Negative'): 'Mixed Happy-Disgusted-Negative',\n",
    "\n",
    "         # Mixed Emotions with Angry face\n",
    "        ('Angry', 'Neutral', 'Positive'): 'Mixed Angry-Neutral-Positive',\n",
    "        ('Angry', 'Sad', 'Positive'): 'Mixed Angry-Sad-Positive',\n",
    "        ('Angry', 'Angry', 'Positive'): 'Mixed Angry-Angry-Positive',\n",
    "        ('Angry', 'Fearful', 'Positive'): 'Mixed Angry-Fearful-Positive',\n",
    "        ('Angry', 'Happy', 'Positive'): 'Mixed Angry-Happy-Positive',\n",
    "        ('Angry', 'Disgusted', 'Positive'): 'Mixed Angry-Disgusted-Positive',\n",
    "        ('Angry', 'Neutral', 'Neutral'): 'Mixed Angry-Neutral-Neutral',\n",
    "        ('Angry', 'Sad', 'Neutral'): 'Mixed Angry-Sad-Neutral',\n",
    "        ('Angry', 'Angry', 'Neutral'): 'Mixed Angry-Angry-Neutral',\n",
    "        ('Angry', 'Fearful', 'Neutral'): 'Mixed Angry-Fearful-Neutral',\n",
    "        ('Angry', 'Happy', 'Neutral'): 'Mixed Angry-Happy-Neutral',\n",
    "        ('Angry', 'Disgusted', 'Neutral'): 'Mixed Angry-Disgusted-Neutral',\n",
    "        ('Angry', 'Neutral', 'Negative'): 'Mixed Angry-Neutral-Negative',\n",
    "        ('Angry', 'Sad', 'Negative'): 'Mixed Angry-Sad-Negative',\n",
    "        ('Angry', 'Fearful', 'Negative'): 'Mixed Angry-Fearful-Negative',\n",
    "        ('Angry', 'Happy', 'Negative'): 'Mixed Angry-Happy-Negative',\n",
    "        ('Angry', 'Disgusted', 'Negative'): 'Mixed Angry-Disgusted-Negative',\n",
    "\n",
    "       # Mixed Emotions with Disgust face\n",
    "        ('Disgust', 'Neutral', 'Positive'): 'Mixed Disgust-Neutral-Positive',\n",
    "        ('Disgust', 'Sad', 'Positive'): 'Mixed Disgust-Sad-Positive',\n",
    "        ('Disgust', 'Angry', 'Positive'): 'Mixed Disgust-Angry-Positive',\n",
    "        ('Disgust', 'Fearful', 'Positive'): 'Mixed Disgust-Fearful-Positive',\n",
    "        ('Disgust', 'Happy', 'Positive'): 'Mixed Disgust-Happy-Positive',\n",
    "        ('Disgust', 'Disgusted', 'Positive'): 'Mixed Disgust-Disgusted-Positive',\n",
    "        ('Disgust', 'Neutral', 'Neutral'): 'Mixed Disgust-Neutral-Neutral',\n",
    "        ('Disgust', 'Sad', 'Neutral'): 'Mixed Disgust-Sad-Neutral',\n",
    "        ('Disgust', 'Angry', 'Neutral'): 'Mixed Disgust-Angry-Neutral',\n",
    "        ('Disgust', 'Fearful', 'Neutral'): 'Mixed Disgust-Fearful-Neutral',\n",
    "        ('Disgust', 'Happy', 'Neutral'): 'Mixed Disgust-Happy-Neutral',\n",
    "        ('Disgust', 'Disgusted', 'Neutral'): 'Mixed Disgust-Disgusted-Neutral',\n",
    "        ('Disgust', 'Neutral', 'Negative'): 'Mixed Disgust-Neutral-Negative',\n",
    "        ('Disgust', 'Sad', 'Negative'): 'Mixed Disgust-Sad-Negative',\n",
    "        ('Disgust', 'Angry', 'Negative'): 'Mixed Disgust-Angry-Negative',\n",
    "        ('Disgust', 'Fearful', 'Negative'): 'Mixed Disgust-Fearful-Negative',\n",
    "        ('Disgust', 'Happy', 'Negative'): 'Mixed Disgust-Happy-Negative',\n",
    "        \n",
    "        # Mixed Emotions with Fear face\n",
    "        ('Fear', 'Neutral', 'Positive'): 'Mixed Fear-Neutral-Positive',\n",
    "        ('Fear', 'Sad', 'Positive'): 'Mixed Fear-Sad-Positive',\n",
    "        ('Fear', 'Angry', 'Positive'): 'Mixed Fear-Angry-Positive',\n",
    "        ('Fear', 'Fearful', 'Positive'): 'Mixed Fear-Fearful-Positive',\n",
    "        ('Fear', 'Happy', 'Positive'): 'Mixed Fear-Happy-Positive',\n",
    "        ('Fear', 'Disgusted', 'Positive'): 'Mixed Fear-Disgusted-Positive',\n",
    "        ('Fear', 'Neutral', 'Neutral'): 'Mixed Fear-Neutral-Neutral',\n",
    "        ('Fear', 'Sad', 'Neutral'): 'Mixed Fear-Sad-Neutral',\n",
    "        ('Fear', 'Angry', 'Neutral'): 'Mixed Fear-Angry-Neutral',\n",
    "        ('Fear', 'Fearful', 'Neutral'): 'Mixed Fear-Fearful-Neutral',\n",
    "        ('Fear', 'Happy', 'Neutral'): 'Mixed Fear-Happy-Neutral',\n",
    "        ('Fear', 'Disgusted', 'Neutral'): 'Mixed Fear-Disgusted-Neutral',\n",
    "        ('Fear', 'Neutral', 'Negative'): 'Mixed Fear-Neutral-Negative',\n",
    "        ('Fear', 'Sad', 'Negative'): 'Mixed Fear-Sad-Negative',\n",
    "        ('Fear', 'Angry', 'Negative'): 'Mixed Fear-Angry-Negative',\n",
    "        ('Fear', 'Happy', 'Negative'): 'Mixed Fear-Happy-Negative',\n",
    "        ('Fear', 'Disgusted', 'Negative'): 'Mixed Fear-Disgusted-Negative',\n",
    "\n",
    "        # Mixed Emotions with Neutral face\n",
    "        ('Neutral', 'Neutral', 'Positive'): 'Mixed Neutral-Neutral-Positive',\n",
    "        ('Neutral', 'Sad', 'Positive'): 'Mixed Neutral-Sad-Positive',\n",
    "        ('Neutral', 'Angry', 'Positive'): 'Mixed Neutral-Angry-Positive',\n",
    "        ('Neutral', 'Fearful', 'Positive'): 'Mixed Neutral-Fearful-Positive',\n",
    "        ('Neutral', 'Happy', 'Positive'): 'Mixed Neutral-Happy-Positive',\n",
    "        ('Neutral', 'Disgusted', 'Positive'): 'Mixed Neutral-Disgusted-Positive',\n",
    "        ('Neutral', 'Sad', 'Neutral'): 'Mixed Neutral-Sad-Neutral',\n",
    "        ('Neutral', 'Angry', 'Neutral'): 'Mixed Neutral-Angry-Neutral',\n",
    "        ('Neutral', 'Fearful', 'Neutral'): 'Mixed Neutral-Fearful-Neutral',\n",
    "        ('Neutral', 'Happy', 'Neutral'): 'Mixed Neutral-Happy-Neutral',\n",
    "        ('Neutral', 'Disgusted', 'Neutral'): 'Mixed Neutral-Disgusted-Neutral',\n",
    "        ('Neutral', 'Neutral', 'Negative'): 'Mixed Neutral-Neutral-Negative',\n",
    "        ('Neutral', 'Sad', 'Negative'): 'Mixed Neutral-Sad-Negative',\n",
    "        ('Neutral', 'Angry', 'Negative'): 'Mixed Neutral-Angry-Negative',\n",
    "        ('Neutral', 'Fearful', 'Negative'): 'Mixed Neutral-Fearful-Negative',\n",
    "        ('Neutral', 'Happy', 'Negative'): 'Mixed Neutral-Happy-Negative',\n",
    "        ('Neutral', 'Disgusted', 'Negative'): 'Mixed Neutral-Disgusted-Negative',\n",
    "\n",
    "        # Mixed Emotions with Sad face\n",
    "        ('Sad', 'Neutral', 'Positive'): 'Mixed Sad-Neutral-Positive',\n",
    "        ('Sad', 'Sad', 'Positive'): 'Mixed Sad-Sad-Positive',\n",
    "        ('Sad', 'Angry', 'Positive'): 'Mixed Sad-Angry-Positive',\n",
    "        ('Sad', 'Fearful', 'Positive'): 'Mixed Sad-Fearful-Positive',\n",
    "        ('Sad', 'Happy', 'Positive'): 'Mixed Sad-Happy-Positive',\n",
    "        ('Sad', 'Disgusted', 'Positive'): 'Mixed Sad-Disgusted-Positive',\n",
    "        ('Sad', 'Neutral', 'Neutral'): 'Mixed Sad-Neutral-Neutral',\n",
    "        ('Sad', 'Sad', 'Neutral'): 'Mixed Sad-Sad-Neutral',\n",
    "        ('Sad', 'Angry', 'Neutral'): 'Mixed Sad-Angry-Neutral',\n",
    "        ('Sad', 'Fearful', 'Neutral'): 'Mixed Sad-Fearful-Neutral',\n",
    "        ('Sad', 'Happy', 'Neutral'): 'Mixed Sad-Happy-Neutral',\n",
    "        ('Sad', 'Disgusted', 'Neutral'): 'Mixed Sad-Disgusted-Neutral',\n",
    "        ('Sad', 'Neutral', 'Negative'): 'Mixed Sad-Neutral-Negative',\n",
    "        ('Sad', 'Angry', 'Negative'): 'Mixed Sad-Angry-Negative',\n",
    "        ('Sad', 'Fearful', 'Negative'): 'Mixed Sad-Fearful-Negative',\n",
    "        ('Sad', 'Happy', 'Negative'): 'Mixed Sad-Happy-Negative',\n",
    "        ('Sad', 'Disgusted', 'Negative'): 'Mixed Sad-Disgusted-Negative',\n",
    "\n",
    "        # Mixed Emotions with Surprise face\n",
    "        ('Surprise', 'Neutral', 'Positive'): 'Mixed Surprise-Neutral-Positive',\n",
    "        ('Surprise', 'Sad', 'Positive'): 'Mixed Surprise-Sad-Positive',\n",
    "        ('Surprise', 'Angry', 'Positive'): 'Mixed Surprise-Angry-Positive',\n",
    "        ('Surprise', 'Fearful', 'Positive'): 'Mixed Surprise-Fearful-Positive',\n",
    "        ('Surprise', 'Happy', 'Positive'): 'Mixed Surprise-Happy-Positive',\n",
    "        ('Surprise', 'Disgusted', 'Positive'): 'Mixed Surprise-Disgusted-Positive',\n",
    "        ('Surprise', 'Neutral', 'Neutral'): 'Mixed Surprise-Neutral-Neutral',\n",
    "        ('Surprise', 'Sad', 'Neutral'): 'Mixed Surprise-Sad-Neutral',\n",
    "        ('Surprise', 'Angry', 'Neutral'): 'Mixed Surprise-Angry-Neutral',\n",
    "        ('Surprise', 'Fearful', 'Neutral'): 'Mixed Surprise-Fearful-Neutral',\n",
    "        ('Surprise', 'Happy', 'Neutral'): 'Mixed Surprise-Happy-Neutral',\n",
    "        ('Surprise', 'Disgusted', 'Neutral'): 'Mixed Surprise-Disgusted-Neutral',\n",
    "        ('Surprise', 'Neutral', 'Negative'): 'Mixed Surprise-Neutral-Negative',\n",
    "        ('Surprise', 'Sad', 'Negative'): 'Mixed Surprise-Sad-Negative',\n",
    "        ('Surprise', 'Angry', 'Negative'): 'Mixed Surprise-Angry-Negative',\n",
    "        ('Surprise', 'Fearful', 'Negative'): 'Mixed Surprise-Fearful-Negative',\n",
    "        ('Surprise', 'Happy', 'Negative'): 'Mixed Surprise-Happy-Negative',\n",
    "        ('Surprise', 'Disgusted', 'Negative'): 'Mixed Surprise-Disgusted-Negative',\n",
    "\n",
    "        \n",
    "        # Add other combinations for Neutral-Neutral-X and X-Neutral-Neutral if needed\n",
    "\n",
    "    }\n",
    "\n",
    "    # Check if the input combination exists in the mapping\n",
    "    key = (face_emotion, voice_emotion, text_sentiment)\n",
    "    if key in emotion_mapping:\n",
    "        return emotion_mapping[key]\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "face_numeric_to_emotion = {\n",
    "    0: 'Angry',\n",
    "    1: 'Disgust',\n",
    "    2: 'Fear',\n",
    "    3: 'Happy',\n",
    "    4: 'Neutral',\n",
    "    5: 'Sad',\n",
    "    6: 'Surprise'\n",
    "}\n",
    "\n",
    "voice_numeric_to_emotion = {\n",
    "    0 : 'Neutral',\n",
    "    1 : 'Happy' ,\n",
    "    2 : 'Sad', \n",
    "    3 : 'Angry', \n",
    "    4 : 'Fearful'   , \n",
    "    5 : 'Disgusted'\n",
    "}\n",
    "\n",
    "text_mapping = {\n",
    "    'neu' : 'Neutral',\n",
    "    'pos' : 'Positive',\n",
    "    'neg' : 'Negative'\n",
    "}\n",
    "\n",
    "for i in range(4):\n",
    "    face_input = final_df['face_senti'][i]\n",
    "    if face_input in face_numeric_to_emotion:\n",
    "        face_input = face_numeric_to_emotion[face_input]\n",
    "    else:\n",
    "        face_input ='Neutrtal'\n",
    "\n",
    "    voice_input = final_df['voice'][i]\n",
    "    if voice_input in voice_numeric_to_emotion:\n",
    "        voice_input = voice_numeric_to_emotion[voice_input]\n",
    "    else:\n",
    "        voice_input ='Neutrtal'\n",
    "\n",
    "\n",
    "    text_input = final_df['text_sentiment'][i]\n",
    "    if text_input in text_mapping:\n",
    "        text_input = text_mapping[text_input]\n",
    "    else:\n",
    "        text_input ='Neutrtal'\n",
    "\n",
    "\n",
    "    print(predict_emotion(face_input,voice_input, text_input))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_fusion(softmax1, softmax2, sentiment_scores, weights):\n",
    "    # Normalize softmax outputs\n",
    "    softmax1_normalized = softmax1 / np.sum(softmax1)\n",
    "    softmax2_normalized = softmax2 / np.sum(softmax2)\n",
    "    \n",
    "    # Normalize sentiment scores\n",
    "    sentiment_normalized = {}\n",
    "    sentiment_sum = sum(sentiment_scores.values())\n",
    "    for key, value in sentiment_scores.items():\n",
    "        sentiment_normalized[key] = value / sentiment_sum\n",
    "    \n",
    "    # Combine the predictions\n",
    "    fusion = softmax1_normalized * weights[0] + softmax2_normalized * weights[1]\n",
    "    for key in sentiment_normalized:\n",
    "        fusion += sentiment_normalized[key] * weights[2]\n",
    "    \n",
    "    return fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
